{"/cloud_platform/getting_started":{"title":"Getting Started with Kaizen Cloud Platform","data":{"":"Welcome to the Kaizen Cloud Platform! This guide will help you set up and start using our AI-powered code review and testing services.","introduction-video#Introduction Video":"Before we dive into the setup process, watch this short introduction video to get an overview of the Kaizen Cloud Platform:","step-1-sign-up-for-an-account#Step 1: Sign Up for an Account":"Visit https://beta.cloudcode.ai\nClick on the \"Sign Up\" button in the top right corner\nFill out the registration form with your details\nVerify your email address by clicking the link sent to your inbox","step-2-create-a-new-project#Step 2: Create a New Project":"Once you're logged in:\nClick on the \"New Project\" button on your dashboard\nChoose a name for your project\nSelect the repository you want to connect (GitHub, GitLab, or Bitbucket)\nFollow the prompts to authorize Kaizen to access your repository","step-3-configure-kaizen-bot#Step 3: Configure Kaizen bot":"Install the kaizen bot by going to this link: \nCheck the permission which kaizen bot requires.\nSelect repositories you want to install kaizen bot.","next-steps#Next Steps":"Kaizen bot should start working whenever you create a PR.Need help? Don't hesitate to contact our support team or join our community Discord for assistance."}},"/configuration":{"title":"Configuration Guide","data":{"":"This document explains the structure and options available in the config.json file, which is used to configure our project.","overview#Overview":"The config.json file is divided into two main sections:\nlanguage_model: Configures the AI language model settings.\ngithub_app: Configures the GitHub app integration settings.\nYou need to store this in the root folder from where you call the kaizen module.","language-model-configuration#Language Model Configuration":"The language_model section contains the following fields:","general-settings#General Settings":"provider: Specifies the provider for the language model (e.g., \"litellm\").\nenable_observability_logging: Boolean flag to enable or disable observability logging.\nredis_enabled: Boolean flag to enable or disable Redis. Used for load balancing multiple models.\nSample Config config.json:\n{\n    \"language_model\": {\n        \"provider\": \"litellm\",\n        \"enable_observability_logging\": true,\n        \"redis_enabled\": true,\n        \"models\": [\n            {\n                \"model_name\": \"default\",\n                \"litellm_params\": {\n                    \"model\": \"gpt-3.5-turbo-1106\",\n                    \"input_cost_per_token\": 0.0000005,\n                    \"output_cost_per_token\": 0.0000015\n                }\n            },\n            {\n                \"model_name\": \"best\",\n                \"litellm_params\": {\n                    \"model\": \"gpt-4o\",\n                    \"input_cost_per_token\": 0.000005,\n                    \"output_cost_per_token\": 0.000015\n                }\n            },\n            {\n                \"model_name\": \"CUSTOM_MODEL\",\n                \"litellm_params\": {\n                    \"model\": \"azure_ai/MODEL_NAME\",\n                    \"api_key\": \"os.environ/CUSTOM_API_KEY\",\n                    \"api_base\": \"os.environ/CUSTOM_API_BASE\"\n                },\n                \"model_info\": {\n                    \"max_tokens\": 4096,\n                    \"input_cost_per_token\": 0.000015,\n                    \"output_cost_per_token\": 0.000015,\n                    \"max_input_tokens\": 128000,\n                    \"max_output_tokens\": 4096,\n                    \"litellm_provider\": \"openai\",\n                    \"mode\": \"chat\"\n                }\n            }\n        ]\n    },\n    \"github_app\": {\n        \"check_signature\": false,\n        \"auto_pr_review\": true,\n        \"edit_pr_desc\": true,\n        \"process_on_push\": true,\n        \"auto_unit_test_generation\": false\n    }\n}","models#Models":"The models array contains configurations for various language models. Each model configuration has the following structure:\n{\n  \"model_name\": \"string\",\n  \"litellm_params\": {\n    \"model\": \"string\",\n    \"input_cost_per_token\": number,\n    \"output_cost_per_token\": number\n  },\n  \"model_info\": { // Optional\n    // Additional model information\n  }\n}\nKey components:\nmodel_name: A custom identifier you assign to the model. You can have multiple configurations with the same model_name, which is useful for routing purposes.\nlitellm_params.model: The official model identifier used by the provider. For example, Azure's GPT-4 might be referenced as azure/gpt-4o.\nlitellm_params.input_cost_per_token and litellm_params.output_cost_per_token: Specify the cost per token for input and output respectively.\nmodel_info: An optional object for additional model-specific information.\nThis flexible structure allows you to define and manage multiple model configurations, including different versions or providers for the same model type.","default-model#Default Model":"{\n  \"model_name\": \"default\",\n  \"litellm_params\": {\n    \"model\": \"gpt-3.5-turbo-1106\",\n    \"input_cost_per_token\": 0.0000005,\n    \"output_cost_per_token\": 0.0000015\n  }\n}\nThis configuration sets up the default model, which is a GPT-3.5 Turbo variant.","best-model#Best Model":"{\n  \"model_name\": \"best\",\n  \"litellm_params\": {\n    \"model\": \"gpt-4o\",\n    \"input_cost_per_token\": 0.000005,\n    \"output_cost_per_token\": 0.000015\n  }\n}\nThis configuration sets up the \"best\" model, which is a GPT-4 variant.","custom-model#Custom Model":"{\n  \"model_name\": \"CUSTOM_MODEL\",\n  \"litellm_params\": {\n    \"model\": \"azure_ai/MODEL_NAME\",\n    \"api_key\": \"os.environ/CUSTOM_API_KEY\",\n    \"api_base\": \"os.environ/CUSTOM_API_BASE\"\n  },\n  \"model_info\": {\n    \"max_tokens\": 4096,\n    \"input_cost_per_token\": 0.000015,\n    \"output_cost_per_token\": 0.000015,\n    \"max_input_tokens\": 128000,\n    \"max_output_tokens\": 4096,\n    \"litellm_provider\": \"openai\",\n    \"mode\": \"chat\"\n  }\n}\nThis configuration sets up a custom model. The CUSTOM_API_KEY and CUSTOM_API_BASE are retrieved from environment variables.","github-app-configuration#GitHub App Configuration":"The github_app section configures the behavior of the GitHub app integration:\n{\n  \"check_signature\": false,\n  \"auto_pr_review\": true,\n  \"edit_pr_desc\": true,\n  \"process_on_push\": true,\n  \"auto_unit_test_generation\": false\n}\ncheck_signature: Boolean flag to enable or disable signature checking.\nauto_pr_review: Boolean flag to enable or disable automatic PR reviews.\nedit_pr_desc: Boolean flag to allow editing of PR descriptions.\nprocess_on_push: Boolean flag to enable processing on push events.\nauto_unit_test_generation: Boolean flag to enable automatic unit test generation.","customizing-the-configuration#Customizing the Configuration":"To customize the configuration:\nCopy the config.json file to your project root.\nModify the values according to your needs.\nEnsure that any referenced environment variables (e.g., CUSTOM_API_KEY) are properly set in your environment.\nRemember to restart your application after making changes to the configuration file for the changes to take effect."}},"/contact_us":{"title":"Contact Us","data":{"":"If you have any questions or need further assistance, please feel free to contact us at support@cloudcode.ai.You can also schedule a demo call here, or connect with any founder on their social media:Demo Call: https://cloudcode.ai/book-a-demo.htmlSaurav Panda\nhttps://github.com/sauravpanda\nhttps://www.linkedin.com/in/pandasaurav/Shreyash Gupta\nhttps://github.com/shreyashkgupta\nhttps://www.linkedin.com/in/shreyashkgupta/"}},"/contributing/code_of_conduct":{"title":"Code of Conduct","data":{"our-pledge#Our Pledge":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.","our-standards#Our Standards":"Examples of behavior that contributes to creating a positive environment include:\nUsing welcoming and inclusive language\nBeing respectful of differing viewpoints and experiences\nGracefully accepting constructive criticism\nFocusing on what is best for the community\nShowing empathy towards other community members\nExamples of unacceptable behavior by participants include:\nThe use of sexualized language or imagery and unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others' private information, such as a physical or electronic address, without explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting","enforcement#Enforcement":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at oss@cloudcode.ai . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately."}},"/contributing/development_setup":{"title":"Kaizen Development Setup Guide","data":{"":"This guide will walk you through the process of setting up the Kaizen development environment.","step-1-clone-the-repository#Step 1: Clone the Repository":"First, clone the Kaizen repository from GitHub:\ngit clone https://github.com/your-username/kaizen.git\ncd kaizen","step-2-set-up-environment-variables#Step 2: Set Up Environment Variables":"Copy the .env.example file to create a new .env file:\ncp .env.example .env\nMake sure to fill in the necessary environment variables in the .env file.","step-3-configure-configjson#Step 3: Configure config.json":"Create a config.json file in the root directory of the project. The general structure should look like this:\n{\n    \"language_model\": {\n        \"provider\": \"litellm\",\n        \"enable_observability_logging\": true,\n        \"redis_enabled\": true,\n        \"models\": [\n            {\n                \"model_name\": \"default\",\n                \"litellm_params\": {\n                    \"model\": \"gpt-4o-mini\",\n                    \"input_cost_per_token\": 0.000000015,\n                    \"output_cost_per_token\": 0.0000006\n                }\n            },\n            {\n                \"model_name\": \"best\",\n                \"litellm_params\": {\n                    \"model\": \"gpt-4o\",\n                    \"input_cost_per_token\": 0.000005,\n                    \"output_cost_per_token\": 0.000015\n                }\n            },\n            {\n                \"model_name\": \"CUSTOM_MODEL\",\n                \"litellm_params\": {\n                    \"model\": \"azure_ai/MODEL_NAME\",\n                    \"api_key\": \"os.environ/CUSTOM_API_KEY\",\n                    \"api_base\": \"os.environ/CUSTOM_API_BASE\"\n                },\n                \"model_info\": {\n                    \"max_tokens\": 4096,\n                    \"input_cost_per_token\": 0.000015,\n                    \"output_cost_per_token\": 0.000015,\n                    \"max_input_tokens\": 128000,\n                    \"max_output_tokens\": 4096,\n                    \"litellm_provider\": \"openai\",\n                    \"mode\": \"chat\"\n                }\n            }\n        ]\n    },\n    \"github_app\": {\n        \"check_signature\": false,\n        \"auto_pr_review\": true,\n        \"edit_pr_desc\": true,\n        \"process_on_push\": true,\n        \"auto_unit_test_generation\": false\n    }\n}","configuration-notes#Configuration Notes:":"model_name is the type of model used for routing. You can have multiple models with the same model_name.\nlitellm_params.model is the actual model name by provider. Refer to the LiteLLM documentation for more details.\nAll API keys should be set up in the .env file.\nYou can set up custom models with custom API keys and base names.","step-4-running-examples#Step 4: Running Examples":"Once the setup is complete, you can run any example using the following command:\nPYTHONPATH=. poetry run python examples/basic/generate.py","development-and-testing#Development and Testing":"To test any development changes:\nFeel free to update the examples as they use the local Kaizen files.\nCreate your own tests in the tests directory.","additional-resources#Additional Resources":"LiteLLM Documentation\nKaizen GitHub Repository\nFor more information or if you encounter any issues, please refer to the project's documentation or create an issue on the GitHub repository."}},"/contributing/how_to_contribute":{"title":"How to Contribute to Our Open Source Project","data":{"":"We're excited that you're interested in contributing to our project! This guide will help you get started with contributing to Kaizen.","table-of-contents#Table of Contents":"Getting Started\nSetting Up Your Development Environment\nFinding Issues to Work On\nMaking Changes\nSubmitting a Pull Request\nCode Review Process\nCommunity Guidelines","getting-started#Getting Started":"Fork the Repository: Start by forking our repository to your GitHub account.\nClone Your Fork: Clone your fork to your local machine:\ngit clone https://github.com/your-username/project-name.git\ncd project-name\nAdd Upstream Remote: Add the original repository as an upstream remote:\ngit remote add upstream https://github.com/Cloud-Code-AI/kaizen.git","setting-up-your-development-environment#Setting Up Your Development Environment":"Install Dependencies: Follow the instructions in our README.md to install necessary dependencies.\npoetry install\nCreate a Branch: Create a new branch for your work:\ngit checkout -b feature/your-feature-name","finding-issues-to-work-on#Finding Issues to Work On":"Check our Issues page for open issues.\nLook for issues tagged with good first issue or help wanted.\nIf you have an idea for a new feature, open an issue to discuss it before starting work.","making-changes#Making Changes":"Write Your Code: Make your changes, following our coding standards and guidelines.\nWrite Tests: Add or update tests as necessary.\nRun Tests: Ensure all tests pass:\npytest .\nCommit Your Changes: Use clear and concise commit messages:\ngit commit -m \"Add feature: brief description of changes\"","submitting-a-pull-request#Submitting a Pull Request":"Push Your Changes: Push your branch to your fork:\ngit push origin feature/your-feature-name\nOpen a Pull Request: Go to the original repository on GitHub and open a pull request.\nDescribe Your Changes: In the PR description, explain your changes and link to any relevant issues.","code-review-process#Code Review Process":"Maintainers will review your PR and may request changes.\nAddress any comments or requested changes.\nOnce approved, a maintainer will merge your PR.","community-guidelines#Community Guidelines":"Be respectful and inclusive in your interactions with other contributors.\nFollow our Code of Conduct.\nParticipate in discussions and help other contributors.","additional-resources#Additional Resources":"Project Documentation\nCoding Standards\nJoin Our Community Chat\nThank you for contributing to our project! Your efforts help make our software better for everyone."}},"/contributing/overview":{"title":"Contributing to Our Project","data":{"":"We're thrilled that you're interested in contributing to our project! This section will guide you through the process of making contributions.","table-of-contents#Table of Contents":"Code of Conduct\nHow to Contribute\nDevelopment Setup\nPull Request Process\nStyle Guide\nWe welcome contributions of all kinds, from bug fixes to new features. Every contribution, no matter how small, is valuable and appreciated.Please take a moment to review this document to make the contribution process easy and effective for everyone involved."}},"/contributing/pull_request_process":{"title":"Pull Request Process","data":{"":"This document outlines the process for submitting a pull request to our project.","before-submitting-a-pull-request#Before Submitting a Pull Request":"Ensure any install or build dependencies are removed before the end of the layer when doing a build.\nUpdate the README.md with details of changes to the interface, this includes new environment variables, exposed ports, useful file locations and container parameters.\nIncrease the version numbers in any examples files and the README.md to the new version that this Pull Request would represent.","submitting-a-pull-request#Submitting a Pull Request":"Fork the repository and create your branch from main.\nIf you've added code that should be tested, add tests.\nIf you've changed APIs, update the documentation.\nEnsure the test suite passes.\nMake sure your code lints.\nIssue that pull request!\nOptionally add your social so that we can tag you when we share an update.","after-submitting-a-pull-request#After Submitting a Pull Request":"The maintainers will review your PR within a few days.\nThey may ask for changes or improvements.\nOnce approved, your PR will be merged into the main branch.\nRemember, the better you describe your pull request and the changes it introduces, the easier it will be for maintainers to review and merge it."}},"/contributing/setting_various_llms":{"title":"Setting up Language Models in config.json","data":{"":"To use various Language Models (LLMs) in your project, you need to configure them in the config.json file. This file should be located in the root directory of your project. Here's how you can set up different LLMs in the config.json file.","basic-setup#Basic Setup":"The config.json file should have the following structure:\n{\n    \"language_model\": {\n        \"provider\": \"litellm\",\n        \"enable_observability_logging\": true,\n        \"redis_enabled\": true,\n        \"models\": [\n            // Model configurations go here\n        ]\n    },\n    \"github_app\": {\n        \"check_signature\": false,\n        \"auto_pr_review\": true,\n        \"edit_pr_desc\": true,\n        \"process_on_push\": true,\n        \"auto_unit_test_generation\": false\n    }\n}\nThe language_model object contains the configurations for the LLMs you want to use. The models array inside it is where you define the configurations for each LLM.","example-open-ai-models#Example: Open AI models":"First, you need to add OPENAI_API_KEY in the .env file.\nHere's an example of how to configure a LitELLM model:\n{\n    \"model_name\": \"default\",\n    \"litellm_params\": {\n        \"model\": \"gpt-4o-mini\",\n        \"input_cost_per_token\": 0.000000015,\n        \"output_cost_per_token\": 0.0000006\n    }\n}\nIn this example, we're defining a model named default that uses the gpt-4o-mini model from LitELLM. The litellm_params object specifies the cost per token for input and output.","example-azure-openai-model#Example: Azure OpenAI Model":"For this example, you need to add the AZURE_API_BASE and AZURE_API_KEY in the .env file.\nTo configure an Azure OpenAI model, you can use the following structure:\n{\n    \"model_name\": \"CUSTOM_MODEL\",\n    \"litellm_params\": {\n        \"model\": \"azure_ai/MODEL_NAME\",\n        \"api_key\": \"os.environ['AZURE_API_KEY']\",\n        \"api_base\": \"os.environ['AZURE_API_BASE']\"\n    },\n    \"model_info\": {\n        \"max_tokens\": 4096,\n        \"input_cost_per_token\": 0.000015,\n        \"output_cost_per_token\": 0.000015,\n        \"max_input_tokens\": 128000,\n        \"max_output_tokens\": 4096,\n        \"litellm_provider\": \"openai\",\n        \"mode\": \"chat\"\n    }\n}","example-groq-models#Example: Groq models":"For GroQ, you need to add the GROQ_API_KEY in the .env file.\nTo configure a GroQ model, you can use the following structure:\n{\n    \"model_name\": \"default\",\n    \"litellm_params\": {\n        \"model\": \"groq/llama3-8b-8192\"\n    }\n}"}},"/contributing/style_guide":{"title":"Style Guide","data":{"":"To ensure consistency across the project, we follow these style guidelines:","code-style#Code Style":"We use Prettier for code formatting. Please ensure your code is formatted before submitting a PR.\nWe follow the Airbnb JavaScript Style Guide for JavaScript code.","commit-messages#Commit Messages":"We use the Conventional Commits specification for commit messages:\nfeat: for new features\nfix: for bug fixes\ndocs: for documentation changes\nstyle: for changes that do not affect the meaning of the code\nrefactor: for code changes that neither fix a bug nor add a feature\nperf: for code changes that improve performance\ntest: for adding or modifying tests\nchore: for changes to the build process or auxiliary tools","documentation-style#Documentation Style":"Use MDX for documentation files.\nKeep language clear and concise.\nUse code blocks for examples.\nInclude links to relevant sections or external resources.","cssscss#CSS/SCSS":"Use kebab-case for class names (e.g., .my-class-name).\nAvoid using IDs for styling.\nUse variables for colors, fonts, and other repeated values.\nRemember, these guidelines are here to help maintain consistency, but they're not set in stone. If you have a good reason to deviate from them, please explain in your pull request."}},"/feature_request":{"title":"Feature Requests","data":{"":"We welcome feature requests from our users, as they help us improve our product and provide a better experience for everyone. If you have an idea for a new feature or an improvement to an existing one, please follow the steps below to submit a feature request.","step-1-check-existing-requests#Step 1: Check Existing Requests":"Before creating a new feature request, we recommend checking the existing ones in our GitHub Discussions to see if your idea has already been suggested. If you find a similar request, you can upvote it or add your thoughts to the existing discussion.","step-2-create-a-new-discussion#Step 2: Create a New Discussion":"If your feature request is unique, you can create a new discussion by following these steps:\nGo to the GitHub Discussions page for our repository.\nClick on the \"New Discussion\" button.\nSelect the \"Feature Request\" category from the dropdown menu.\nProvide a clear and descriptive title for your feature request.\nIn the discussion body, provide a detailed description of your proposed feature or enhancement. Include the following information:\nA brief overview of the feature\nThe problem it solves or the benefit it provides\nAny relevant use cases or scenarios\nMockups, diagrams, or examples (if applicable)\nAny additional context or information that could be helpful","step-3-engage-with-the-community#Step 3: Engage with the Community":"Once you've created your feature request, others in the community can view, comment, and react to it. We encourage you to engage with the community, answer questions, and provide additional details or clarifications as needed.","step-4-wait-for-review-and-feedback#Step 4: Wait for Review and Feedback":"Our team regularly reviews the feature requests in the discussions. We may ask for additional information, provide feedback, or share our plans regarding the implementation of the feature. Please be patient, as we carefully evaluate each request and prioritize them based on various factors.","step-5-stay-informed#Step 5: Stay Informed":"We will update the discussion with any progress or decisions made regarding your feature request. If your request is accepted for implementation, we will provide updates on the development timeline and release plans.Thank you for your contributions and for helping us make our product better!"}},"/features/code_review":{"title":"Code Review","data":{"code-review-by-kaizen#Code Review by Kaizen":"The Code Review is an AI-powered tool that automatically reviews and provides feedback on code changes in your pull requests (PRs). It helps streamline the code review process and ensures high code quality across your codebase.","how-it-works#How it Works":"Diff Analysis: When you create or update a pull request, the Code Review Bot analyzes the code changes (diff) and generates detailed feedback based on the modified code snippets.\nOrganized Feedback: The bot's feedback is organized into topics or categories like performance, security, code style, or documentation, making it easier to navigate and prioritize the comments.\nConfidence Levels: Each review comment includes a confidence level (critical, high, medium, low), indicating the perceived importance or severity of the issue.\nContextual Information: The reviews provide context-specific details like file names, line numbers, code snippets, and explanations for the suggested changes.\nPR Description Generation: The bot can generate a descriptive summary of the code changes, helping you better document your pull requests.","using-the-code-review-bot#Using the Code Review Bot":"Create or Update a Pull Request: The Code Review Bot will automatically analyze the code changes and generate a review.\nReview the Feedback: The bot's feedback will be shared as a comment on your pull request, organized by topics and confidence levels.\nEngage with the Bot: You can interact with the bot, provide additional context, or request clarification on its feedback.\nIterate and Improve: As you work with the bot, it will learn from your responses and improve the quality of its reviews over time.","benefits#Benefits":"Improved Code Quality: Catch potential issues and receive suggestions for improvements early in the development process.\nTime Savings: Automated reviews reduce the time and effort required for manual code reviews.\nConsistent Standards: Ensure consistent application of coding standards, best practices, and guidelines across your codebase.\nKnowledge Sharing: The bot's reviews serve as a knowledge-sharing mechanism, providing insights that can benefit your entire development team.","limitations#Limitations":"AI Limitations: While advanced, the bot may still have limitations in understanding complex code or context-specific nuances.\nHuman Oversight: The bot's feedback should be considered a complementary tool to human code reviews, not a complete replacement.\nWith the Code Review Bot, you can streamline your code review process, maintain high code quality, and leverage the power of AI to enhance your development workflow."}},"/features/code_scan":{"title":"Code Scan","data":{"code-scanner#Code Scanner":"The Code Scanner feature is powered by kaizen and provides comprehensive static code analysis to identify potential issues, vulnerabilities, and areas for improvement in your codebase.","how-it-works#How it Works:":"Input your source code or provide access to your repository.\nThe Code Scanner employs advanced algorithms and AI to analyze the code and generate a detailed report of findings.\nYou can find an example here","using-the-code-scanner#Using the Code Scanner:":"Submit your code or repository for analysis.\nTrigger the Code Scanner to perform a thorough examination.\nReview the generated report highlighting issues, potential bugs, and suggestions for improvement.","benefits#Benefits:":"Early Bug Detection\nSecurity Vulnerability Identification\nCode Quality Improvement\nCoding Standard Enforcement\nTechnical Debt Reduction","limitations#Limitations:":"False Positives: Some identified issues may not be actual problems in certain contexts.\nLanguage Coverage: Effectiveness may vary depending on programming language and framework.\nThe Code Scanner utilizes AI and static analysis techniques to enhance code quality, security, and maintainability, supporting a proactive approach to software development and maintenance."}},"/features/e2e_testing":{"title":"E2e Testing","data":{"e2e-ui-testing-with-playwright#E2E UI Testing with Playwright":"The E2E UI Testing feature is designed to streamline the process of creating and maintaining comprehensive end-to-end tests for web applications using the Playwright testing framework. It leverages advanced language models to generate robust and maintainable test scripts that can be seamlessly integrated into a CI/CD pipeline.","key-features#Key Features":"Test Plan Generation: The feature analyzes the application requirements or specifications and automatically generates a comprehensive test plan, covering various user flows and scenarios.\nPlaywright Test Script Generation: Based on the test plan, the feature generates Playwright test scripts written in Python 3.9, following best practices and industry standards.\nPage Object Model: The generated scripts implement the Page Object Model (POM) design pattern, promoting code reusability, maintainability, and separation of concerns.\nWeb Element Interaction: The scripts leverage Playwright's powerful features for interacting with web elements, such as clicking buttons, filling out forms, and navigating between pages.\nVisual Testing: The feature utilizes Playwright's capabilities to capture screenshots and videos during test execution, enabling visual validation and debugging.","usage#Usage":"Provide Web URL: To generate E2E UI tests, provide the URL of the web application you want to test.\nTest Generation: The feature will analyze the web application's content, identify different UI modules, and generate appropriate Playwright test scripts for each module.\nTest Execution: The generated test scripts can be executed locally or integrated into a CI/CD pipeline for automated testing during the development lifecycle.\nTest Reporting: After test execution, the feature provides detailed reports, including test results, captured screenshots, and videos for failed tests, facilitating debugging and issue resolution.\nContinuous Integration: As the web application evolves, the feature can regenerate or update the test scripts to ensure they remain aligned with the latest changes and requirements.\nYou can find an example here","benefits#Benefits":"Accelerated Test Development: By leveraging advanced language models, the feature significantly accelerates the process of creating comprehensive E2E UI tests.\nMaintainability: The generated scripts follow best practices and industry standards, promoting code maintainability and extensibility.\nReusability: The implementation of the Page Object Model design pattern enhances code reusability and modularization.\nContinuous Quality Assurance: The seamless integration with CI/CD pipelines enables continuous testing and quality assurance throughout the software development lifecycle.\nVisual Validation: The ability to capture screenshots and videos during test execution aids in visual validation and issue identification.\nScalability: The feature supports data-driven testing and parallelization strategies, enabling scalable and efficient test execution.\nWith the E2E UI Testing feature, you can streamline the creation and maintenance of robust end-to-end tests for your web applications, ensuring comprehensive coverage, accelerating the testing process, and fostering a culture of continuous quality assurance within your development workflow"}},"/features/ui_review":{"title":"UI Review","data":{"ui-reviewer#UI Reviewer":"The UI Reviewer feature is powered by kaizen and provides feedback to improve the user experience and accessibility of UI components.","how-it-works#How it Works:":"Input the HTML code of your UI components.\nThe UI Reviewer leverages advanced language models to analyze the code and generate actionable feedback organized in a JSON format.\nYou can find an example here","using-the-ui-reviewer#Using the UI Reviewer:":"Gather and input the HTML code of the UI components you want to review.\nTrigger the UI Reviewer to receive organized feedback.\nImplement the suggested solutions to enhance user experience and accessibility.","benefits#Benefits:":"Improved User Experience\nAccessibility Compliance\nStreamlined Review Process\nContinuous Improvement","limitations#Limitations:":"AI Limitations: Understanding complex UI patterns or context-specific nuances.\nHuman Oversight: AI feedback should complement human review and testing.\nThe UI Reviewer uses AI to enhance the user experience and accessibility of UI components and fosters continuous improvement in your development workflow."}},"/features/unit_test":{"title":"Unit Test Generator","data":{"":"The Unit Test Generator feature is powered by kaizen and automatically creates comprehensive unit tests for your code, improving code quality and test coverage.","how-it-works#How it Works:":"Input the source code or directory for which you want to generate unit tests.\nThe Unit Test Generator leverages advanced language models to analyze the code and generate appropriate unit tests in a format compatible with popular testing frameworks.\nThe generator supports multiple programming languages and can handle entire directories of code files.\nYou can find an example here","using-the-unit-test-generator#Using the Unit Test Generator:":"Provide the source code file or directory path for which you want to generate unit tests.\n(Optional) Configure output path, verbosity, and critique settings.\nRun the generator to create unit tests.\nReview and integrate the generated tests into your test suite.","supported-languages#Supported Languages:":"Python (.py)\nJavaScript (.js)\nTypeScript (.ts)\nReact (.jsx, .tsx)\nRust (.rs)","how-to-run#How to Run:":"","installation#Installation":"Before using the Unit Test Generator, you need to install the Kaizen Cloud Code SDK. You can do this using pip:\npip install kaizen-cloudcode","usage-guide#Usage Guide":"Here's a step-by-step guide on how to use the Unit Test Generator:\nImport the UnitTestGenerator:\nfrom kaizen.generator.unit_test import UnitTestGenerator\nCreate an instance of the generator:\ngenerator = UnitTestGenerator()\nGenerate tests for a specific file:\ngenerator.generate_tests(\n    file_path=\"path/to/your/file.py\",\n    enable_critique=True,\n    verbose=True\n)\n(Optional) Run the generated tests:\ntest_results = generator.run_tests()\n(Optional) Display the test results:\nfor file_path, result in test_results.items():\n    print(f\"Results for {file_path}:\")\n    if \"error\" in result:\n        print(f\"  Error: {result['error']}\")\n    else:\n        print(f\"  Tests run: {result.get('tests_run', 'N/A')}\")\n        print(f\"  Failures: {result.get('failures', 'N/A')}\")\n        print(f\"  Errors: {result.get('errors', 'N/A')}\")\n    print()","complete-example#Complete Example:":"Here's a complete example of how to use the Unit Test Generator:\nfrom kaizen.generator.unit_test import UnitTestGenerator\n# Create an instance of the generator\ngenerator = UnitTestGenerator()\n# Generate tests for a specific file\ngenerator.generate_tests(\n    file_path=\"kaizen/helpers/output.py\",\n    enable_critique=True,\n    verbose=True\n)\n# Run the generated tests\ntest_results = generator.run_tests()\n# Display the test results\nfor file_path, result in test_results.items():\n    print(f\"Results for {file_path}:\")\n    if \"error\" in result:\n        print(f\"  Error: {result['error']}\")\n    else:\n        print(f\"  Tests run: {result.get('tests_run', 'N/A')}\")\n        print(f\"  Failures: {result.get('failures', 'N/A')}\")\n        print(f\"  Errors: {result.get('errors', 'N/A')}\")\n    print()","api-reference#API Reference:":"","class-unittestgenerator#Class: UnitTestGenerator":"","constructor#Constructor":"__init__(self, verbose=False)\nInitializes the UnitTestGenerator with optional verbosity setting.","methods#Methods":"","generate_tests_from_dir#generate_tests_from_dir":"generate_tests_from_dir(self, dir_path: str, output_path: str = None)\nGenerates unit tests for all supported files in a given directory.\nParameters:\ndir_path: Path of the directory containing source files.\nmax_critique: Maximum number of critique iterations.\noutput_path: (Optional) Custom output path for generated tests.\nverbose: Enable verbose logging.\nenable_critique: Enable AI critique and improvement of generated tests.\nReturns: A tuple containing an empty dictionary and llm usage statistics.","generate_tests#generate_tests":"generate_tests(self, file_path: str, content: str = None, max_critique: int = 3, output_path: str = None, verbose: bool = False, enable_critique: bool = False)\nGenerates unit tests for a given file with various configuration options.\nParameters:\nfile_path: Path of the file relative to the project root.\ncontent: (Optional) File content.\nmax_critique: Maximum number of critique iterations.\noutput_path: (Optional) Custom output path for generated tests.\nverbose: Enable verbose logging.\nenable_critique: Enable AI critique and improvement of generated tests.\nReturns: A tuple containing an empty dictionary and llm usage statistics.","run_tests#run_tests":"run_tests(self) -> Dict\nRuns the generated unit tests and returns the results.","key-features#Key Features:":"Multi-language support\nDirectory-wide test generation\nAI-powered test scenario creation\nTest critique and improvement\nDetailed logging and progress tracking\nToken usage monitoring","benefits#Benefits:":"Increased Test Coverage\nTime Efficiency\nConsistency in Testing\nEarly Bug Detection\nSupport for Multiple Programming Languages\nContinuous Improvement through AI Critique","limitations#Limitations:":"AI Limitations: May not cover all edge cases or complex scenarios.\nHuman Oversight: Generated tests should be reviewed and potentially modified by developers.\nLanguage Support: Limited to the supported programming languages.","advanced-usage#Advanced Usage:":"Enable critique mode for AI-powered test improvement\nAdjust verbosity for detailed logging\nCustomize output paths for generated tests\nConfigure maximum critique iterations for fine-tuned results\nThe Unit Test Generator uses AI to enhance the testing process, improve code quality, and streamline the development workflow by automating the creation of unit tests across multiple programming languages."}},"/features/work_summary":{"title":"Work Summary Generator","data":{"":"The Work Summary Generator is a tool that creates easy-to-understand summaries of code changes for non-technical stakeholders and founders.","how-it-works#How it works:":"Provide a Git diff representing the code changes to be summarized.\nThe tool uses advanced language models to analyze the diff and generate a detailed summary.\nThe summary includes a high-level overview, sectional breakdown, plain language explanations, feature highlights, impact analysis, and visual aids.\nThe tool ensures a consistent, readable structure.\nYou can find an example here","using-the-work-summary-generator#Using the Work Summary Generator:":"Generate a Git diff representing the code changes.\nTrigger the generator to create a user-friendly summary.\nShare the summary with stakeholders for better communication and understanding.\nUse the summary as a basis for discussions and decision-making.","benefits#Benefits:":"Bridge the gap between technical and non-technical stakeholders.\nKeep stakeholders engaged and informed about development progress.\nProvide a user-friendly way for non-technical stakeholders to track development progress.\nStreamline the reporting process with automatically generated work summaries.\nWith the Work Summary Generator, you can effectively communicate the value and progress of your development efforts to non-technical stakeholders, fostering better collaboration and alignment throughout the project lifecycle."}},"/getting_started":{"title":"Getting Started","data":{"":"You can start using Cloud Code AI in two different ways:","cloud-hosted-solution-recommended#Cloud Hosted Solution (Recommended)":"Utilize our cloud-hosted, scalable application at https://beta.cloudcode.ai to quickly access and leverage Cloud Code AI's capabilities without setting up your own infrastructure. This option is ideal for immediate use and simplifies the deployment process.","self-hosted-deployment#Self-Hosted Deployment":"Deploy your own instance of the open-source Cloud Code project by following the installation instructions provided in our documentation. Follow the steps outlined in the installation guide to set up Cloud Code AI on your infrastructure.Choose the option that best fits your needs and start accelerating your software development with Cloud Code AI today!"}},"/":{"title":"Index","data":{"":"Welcome to the Kaizen documentation! This resource will guide you through utilizing Kaizen to enhance your software development journey — from code reviews and testing automation to ensuring the security and reliability of your applications.","about-kaizen#About Kaizen":"Kaizen is an open-source project that helps teams ensure quality in their software delivery by providing a suite of tools for code review, test generation, and end-to-end testing. It integrates with your existing code repositories and workflows, allowing you to streamline your software development process.","key-features#Key Features":"","end-to-end-testing-wip#End-to-End Testing [WIP]":"Kaizen generates comprehensive end-to-end tests based on your application's code and documentation. These tests ensure that your application functions correctly from start to finish, catching regressions and edge cases that may have been overlooked during development.","ui-testing-and-review#UI Testing and Review":"Kaizen can provide teams with helpful reviews for their UI and generate necessary tests to ensure that their website works as expected.","code-review#Code Review":"Kaizen automatically reviews pull requests, summarizes code changes and provides insightful feedback on potential issues or areas of improvement. It leverages advanced natural language processing techniques to understand the context and implications of the code changes."}},"/self_hosting_guide":{"title":"Self Hosting Guide","data":{"":"This guide outlines the steps to set up a self-hosted instance of CloudCode, which utilizes a GitHub App to perform various actions related to pull requests (PRs) and repository management.CloudCode allows you to run its API server locally or on your own infrastructure.","prerequisites#Prerequisites":"Before you begin, ensure you have the following prerequisites installed and configured on your system:\nGit: Version control system used to clone the CloudCode repository.\nPython: Required for using Poetry, a dependency manager used by CloudCode.\nDocker and Docker Compose: For deploying and managing containerized applications.\nGitHub Account: You'll need this to set up a GitHub App and configure permissions.","outline#Outline":"To create your own local version of CloudCode, follow along the guide to complete the following steps:\nClone the repository.\nInstall required dependencies.\nCreate a Github app.\nPrepare the configuration settings.\nDeploy the API."}},"/self_hosting_guide/clone_repository":{"title":"Clone Repository","data":{"":"Run the following command in your local machine:\ngit clone https://github.com/cloudcode/cloudcode.git\nThis command will clone the CloudCode repository from GitHub to your local machine."}},"/self_hosting_guide/deploy_api":{"title":"Deploy API","data":{"":"To deploy the CloudCode API using Docker Compose, follow these steps:\nMake sure you have Docker and Docker Compose installed on your system.\nNavigate to the CloudCode project directory:\ncd cloudcode\nRun the following command to start the API server and its dependencies using Docker Compose:\ndocker-compose up\nThis command will build and launch the Docker containers defined in the docker-compose.yml file. The API server for CloudCode will be deployed and accessible based on the configurations specified in the docker-compose.yml file.You can monitor the logs and access the API endpoints once the containers are up and running. Use Ctrl + C to stop the Docker Compose process when you're finished."}},"/self_hosting_guide/executing_tests":{"title":"Executing Tests","data":{"":"Once you have generated all the necessary tests, you can run all the tests in two ways:\nRun the following command to execute the tests.\n   PYTHONPATH=. poetry run python examples/basic/execute.py\nOR\nUse the default pytest module to execute all tests.\n    pytest -v .kaizen/tests/\nKaizen will generate all the tests and store them inside .kaizen/tests/"}},"/self_hosting_guide/generating_tests":{"title":"Generating Tests","data":{"":"Follow these steps to generate tests for your website.\nUpdate the URL in the file at examples/basic/generate.py - Line 5.\nRun the following command to generate tests.\n    PYTHONPATH=. poetry run python examples/basic/generate.py\nKaizen will generate all the tests and store them inside .kaizen/tests/"}},"/self_hosting_guide/install_dependencies":{"title":"Install Dependencies","data":{"":"To install dependencies for CloudCode, follow these steps:\nChange directory to the CloudCode project directory:\n   cd cloudcode\nUse Poetry to install the project dependencies:\n   poetry install\nThese commands will navigate you into the CloudCode project directory and then use Poetry to install the required dependencies specified in the project's pyproject.toml file."}},"/self_hosting_guide/prepare_configuration":{"title":"Prepare Configuration","data":{"":"To prepare for deployment, follow these steps to configure the necessary files:\nCopy Environment File:\nCreate a .env file by duplicating .env.example. This file will store environment variables required for the deployment process.\nStore GitHub App PEM File:\nEnsure the PEM file for the GitHub app is stored as GITHUB_APP_KEY.pem. This file is necessary for authentication and interaction with the GitHub platform during deployment.","configuring-features#Configuring Features":"You can adjust additional features by modifying the config.json file. This JSON file controls specific configurations related to the deployment process and application behavior.Make sure to update these files appropriately before proceeding with deployment."}},"/self_hosting_guide/setup_github_app":{"title":"Setup GitHub App","data":{"":"Follow these steps to set up your GitHub app:","registering-your-github-app#Registering Your GitHub App":"Access Developer Settings:\nClick on your profile photo in the upper-right corner of GitHub.\nGo to Settings and then click on Developer settings in the left sidebar.\nCreate a New GitHub App:\nUnder Developer settings, select GitHub Apps and click on New GitHub App.\nProvide App Details:\nEnter the following details for your app:\nGitHub App name: Choose a name for your app.\nHomepage URL: Specify the URL of your app (e.g., repository URL).\nWebhook URL: Enter the server URL where webhook requests will be sent.\nOptionally, set a Webhook secret for request validation.","configure-webhook-and-permissions#Configure Webhook and Permissions":"Activate Webhook:\nEnsure the \"Webhook\" toggle is set to Active.\nSet Permissions:\nIn the sidebar, navigate to Permissions & events.\nDefine Permissions:\nUnder Repository permissions, Organization permissions, and Account permissions, select required permissions such as:\nRepository Content\nPull Request\nMetadata\nCheck Runs","subscribe-to-events#Subscribe to Events":"Choose Events:\nUnder Subscribe to Events, select the webhook events your app needs to receive (e.g., \"Pull request\" events).\nBy following these steps, your GitHub app will be properly configured with the necessary permissions and event subscriptions for your project."}},"/self_hosting_guide/using_azure_openai_with_kaizen":{"title":"How to Integrate Azure OpenAI with Kaizen","data":{"how-to-integrate-azure-openai-with-kaizen#How to Integrate Azure OpenAI with Kaizen?":"This post addresses a great question raised by Edwin regarding the integration of Azure OpenAI with Kaizen. Let's dive into the steps needed to make this integration work seamlessly.","step-1-set-environment-variables#Step 1: Set Environment Variables":"First, you need to add the necessary environment variables in your .env file. These variables are essential for authenticating and interacting with the Azure OpenAI service.\nos.environ[\"AZURE_API_KEY\"] = \"<YOUR_AZURE_API_KEY>\"  \nos.environ[\"AZURE_API_BASE\"] = \"<YOUR_AZURE_API_BASE_URL>\"  \nos.environ[\"AZURE_API_VERSION\"] = \"<YOUR_AZURE_API_VERSION>\"  \nReplace <YOUR_AZURE_API_KEY>, <YOUR_AZURE_API_BASE_URL>, and <YOUR_AZURE_API_VERSION> with your actual Azure OpenAI credentials.","step-2-update-configjson#Step 2: Update config.json":"Next, update your config.json file to include the model configuration for Azure. This configuration will specify which model deployment to use and the associated costs per token.\n{  \n  ...  \n  \"model_config\": {  \n    \"model\": \"azure/<NAME-OF-DEPLOYMENT>\",  \n    \"input_cost_per_token\": 0.000005,  \n    \"output_cost_per_token\": 0.000015  \n  },  \n  ...  \n}  \nReplace <NAME-OF-DEPLOYMENT> with the name of your Azure OpenAI deployment.Feel free to reach out if you have any questions or need further assistance!If you found this post helpful, make sure to check out our other guides and tutorials. Happy coding!"}}}